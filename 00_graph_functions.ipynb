{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp graph_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-presence",
   "metadata": {},
   "source": [
    "# Graph Functions\n",
    "\n",
    "> Operations on networkx MultiDiGraph objects, including assigning and obtaining attributes from nodes and edges, converting edges into nodes, and sorting and labeling graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *\n",
    "from nbdev.imports import *\n",
    "from nbdev.export import *\n",
    "from nbdev.sync import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import warnings\n",
    "with warnings.catch_warnings(): #ignore warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    import networkx as nx\n",
    "    import numpy as np\n",
    "    import sidis\n",
    "    rng=sidis.RNG(0)\n",
    "    import matplotlib.pyplot as plt\n",
    "    import typing\n",
    "    from typing import Optional, Tuple, Dict, Callable, Union, Mapping, Sequence, Iterable, Hashable, List, Any\n",
    "    from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ring(N : int = 3,\n",
    "         left : bool = True,\n",
    "         right : bool = False,\n",
    "         loop : bool = False):\n",
    "    '''\n",
    "    Return `g`, a ring topology networkx graph with `N` nodes.\n",
    "    Booleans `left`, `right`, `loop` determine the directed edges.\n",
    "    '''\n",
    "\n",
    "    g=nx.MultiDiGraph()\n",
    "\n",
    "    e=[]\n",
    "\n",
    "    if left:\n",
    "        e+=[(i,(i-1)%N) for i in range(N)]\n",
    "    if right:\n",
    "        e+=[(i,(i+1)%N) for i in range(N)]\n",
    "    if loop:\n",
    "        e+=[(i,i) for i in range(N)]\n",
    "\n",
    "    g.add_nodes_from([i for i in range(N)])\n",
    "    g.add_edges_from(e)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "g=ring(N=3,left=True,right=True,loop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def table(iterable : Iterable, header : Iterable[str]):\n",
    "    '''\n",
    "    Creates a simple ASCII table from an iterable and a header.\n",
    "    Modified from\n",
    "    https://stackoverflow.com/questions/5909873/how-can-i-pretty-print-ascii-tables-with-python\n",
    "    '''\n",
    "    max_len = [len(x) for x in header]\n",
    "    for row in iterable:\n",
    "        row = [row] if type(row) not in (list, tuple) else row\n",
    "        for index, col in enumerate(row):\n",
    "            if max_len[index] < len(str(col)):\n",
    "                max_len[index] = len(str(col))\n",
    "\n",
    "    output = '|' + ''.join([h + ' ' * (l - len(h)) + '|' for h, l in zip(header, max_len)]) + '\\n'\n",
    "\n",
    "    for row in iterable:\n",
    "        row = [row] if type(row) not in (list, tuple) else row\n",
    "        output += '|' + ''.join([str(c) + ' ' * (l - len(str(c))) + '|' for c, l in zip(row, max_len)]) + '\\n'\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_graph(g : nx.MultiDiGraph,\n",
    "               string=False):\n",
    "    '''\n",
    "    Print the 'node', predecessors', and 'successors' for every node in graph `g`.\n",
    "    The predecessors are the nodes flowing into a node, \n",
    "    and the successors are the nodes flowing out.\n",
    "    \n",
    "    Example use:\n",
    "        g=ring(N=3,left=True,right=True,loop=True)\n",
    "        print_graph(g)\n",
    "    '''\n",
    "    data = [[n, list(g.predecessors(n)), list(g.successors(n))] for n in g.nodes]\n",
    "    for i in range(len(data)):\n",
    "        data[i][1]=', '.join([str(i) for i in data[i][1]])\n",
    "        data[i][2]=', '.join([str(i) for i in data[i][2]])\n",
    "\n",
    "    header=['Node', 'Predecessors', 'Successors']\n",
    "    \n",
    "    if not string:\n",
    "        print(table(data,header))\n",
    "    else:\n",
    "        return table(data,header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Node|Predecessors|Successors|\n",
      "|0   |1, 2, 0     |2, 1, 0   |\n",
      "|1   |2, 0, 1     |0, 2, 1   |\n",
      "|2   |0, 1, 2     |1, 0, 2   |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_kwargs(**kwargs):\n",
    "    '''\n",
    "    Evaluate delayed function calls by replacing \n",
    "        kwarg=(func,*farg,dict(**fkwarg))\n",
    "    with\n",
    "        kwarg=func(*farg,**fkwarg)\n",
    "     \n",
    "    Example: kwargs = {a : (np.random.random,1)}\n",
    "    becomes  kwargs = {a : np.random.random(1)}\n",
    "    each time this function is called.\n",
    "    \n",
    "    Used to randomize kwarg assignment for \n",
    "    an exterior function, e.g setting node\n",
    "    and edge attributes.\n",
    "    '''\n",
    "    newkwargs={k:v for k,v in kwargs.items()}\n",
    "    for k,v in kwargs.items():\n",
    "        if type(v) is tuple and callable(v[0]):\n",
    "            if len(v)==1:\n",
    "                newkwargs[k]=v[0]()\n",
    "            elif len(v)==2:\n",
    "                if type(v[-1]) is dict:\n",
    "                    newkwargs[k]=v[0](**v[1])\n",
    "                else:\n",
    "                    newkwargs[k]=v[0](v[1])\n",
    "            else:\n",
    "                if type(v[-1]) is dict:\n",
    "                    newkwargs[k]=v[0](*v[1:-1],**v[-1])\n",
    "                else:\n",
    "                    newkwargs[k]=v[0](*v[1:])\n",
    "    return newkwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-aruba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init': array([1, 0, 0, 0, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_kwargs(init=(rng.random,dict(x=0,y=1,shape=8,asint=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-entrance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': array([0.54362499, 0.93507242])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_kwargs(a=(rng.random,dict(x=0,y=1,shape=2))) #specify kwargs explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-thermal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.9957327011121017}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_kwargs(a = (np.random.uniform,0,1) ) #specify args implicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-rolling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.2571581456075053}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_kwargs(a = (np.random.uniform,0,dict(high=1)) ) #mix the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-bibliography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.968339261201048}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_kwargs( a = (np.random.random,) ) #tuple of just func calls default func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-madrid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 1, 'a': <function RandomState.random>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_kwargs(b=1,a=np.random.random) #no tuple assigns kwargs normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def give_nodes(g : nx.MultiDiGraph,\n",
    "               data : Dict[Hashable,dict] = None,\n",
    "               nodes : Iterable = None,\n",
    "               **kwargs):\n",
    "    '''\n",
    "    Parse and apply any 'kwargs' to a set of 'nodes'.\n",
    "    If given, 'data' is a dict-of-dicts keyed by node.\n",
    "    The inner dict is given to the corresponding node.\n",
    "    '''\n",
    "    if nodes is None:\n",
    "        nodes=g.nodes\n",
    "    \n",
    "    if kwargs:\n",
    "        [sidis.give(g.nodes[n],**parse_kwargs(**kwargs)) for n in nodes]\n",
    "    \n",
    "    if data:\n",
    "        for k,v in data.items():\n",
    "            try:\n",
    "                g.nodes[k].update(parse_kwargs(**v))\n",
    "            except KeyError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-springer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeDataView({0: {'a': array([0.83597281])}, 1: {'a': array([0.58115496])}, 2: {'a': array([0.00429846])}})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "give_nodes(g,a=(np.random.random,1))\n",
    "g.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-intake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeDataView({0: {'a': array([0.83597281]), 'b': 1}, 1: {'a': array([0.58115496])}, 2: {'a': array([0.00429846])}})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "give_nodes(g,{0:dict(b=1)})\n",
    "g.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-biography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeDataView({0: {'a': array([0.83597281]), 'b': 1}, 1: {'a': array([0.58115496])}, 2: {'a': array([0.00429846]), 'c': 2}})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "give_nodes(g,nodes=[2],c=2)\n",
    "g.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def parse_edges(edges : Union[tuple,List[tuple]], \n",
    "                default_key : Hashable = 0\n",
    "               ):\n",
    "    '''\n",
    "    Parse a single edge or list of edges\n",
    "    into a list of 3-tuples for iterating over \n",
    "    a MultiDiGraph, which requires keys. \n",
    "    '''\n",
    "    if type(edges) is tuple:\n",
    "        edges=[edges]\n",
    "    if type(edges) is not list:\n",
    "        edges=list(edges)\n",
    "    for i in range(len(edges)):\n",
    "        if len(edges[i])==4: #discard data, last entry\n",
    "            edges[i]=(edges[i][0],edges[i][1],edges[i][2])\n",
    "        if len(edges[i])==2: #include key, 3rd entry\n",
    "            edges[i]=(edges[i][0],edges[i][1],default_key)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def give_edges(g : nx.MultiDiGraph,\n",
    "               data : Dict[Hashable,dict] = None,\n",
    "               edges : Iterable = None,\n",
    "               **kwargs):\n",
    "    '''\n",
    "    Parse and apply any 'kwargs' to a set of 'edges'.\n",
    "    If given, 'data' is a dict-of-dicts keyed by edge.\n",
    "    The inner dict is given to the corresponding edge.\n",
    "    '''\n",
    "    if edges is None:\n",
    "        edges=g.edges\n",
    "        \n",
    "    edges = parse_edges(edges)\n",
    "    \n",
    "    if kwargs:\n",
    "        [sidis.give(g.edges[e],**parse_kwargs(**kwargs)) for e in edges]\n",
    "    \n",
    "    if data:\n",
    "        for k,v in data.items():\n",
    "            try:\n",
    "                g.edges[k].update(parse_kwargs(**v))\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-hamilton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutMultiEdgeDataView([(0, 2, {'d': 1}), (0, 1, {'d': 1}), (0, 0, {'d': 1}), (1, 0, {'d': 1}), (1, 2, {'d': 1}), (1, 1, {'d': 1}), (2, 1, {'d': 1}), (2, 0, {'d': 1}), (2, 2, {'d': 1})])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "give_edges(g,d=1)\n",
    "g.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-formula",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutMultiEdgeDataView([(0, 2, {'d': 0}), (0, 1, {'d': 1}), (0, 0, {'d': 2}), (1, 0, {'d': 3}), (1, 2, {'d': 4}), (1, 1, {'d': 5}), (2, 1, {'d': 6}), (2, 0, {'d': 7}), (2, 2, {'d': 8})])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "give_edges(g,{e:dict(d=i) for i,e in enumerate(g.edges)})\n",
    "g.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def node_attrs(g):\n",
    "    '''\n",
    "    Unique node data keys.\n",
    "    '''\n",
    "    attrs=[]\n",
    "    for n in g.nodes:\n",
    "        for attr in g.nodes[n]:\n",
    "            attrs+=[attr]\n",
    "    return list(set(attrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-canvas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c', 'b', 'a']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_attrs(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def edge_attrs(g):\n",
    "    '''\n",
    "    Unique edge data keys.\n",
    "    '''\n",
    "    attrs=[]\n",
    "    for e in g.edges:\n",
    "        for attr in g.edges[e]:\n",
    "            attrs+=[attr]\n",
    "    return list(set(attrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-street",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attrs(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def node_data(g,*args):\n",
    "    '''\n",
    "    Return node attributes 'args' as an array.\n",
    "    NOTE: The ordering of the array corresponds to the\n",
    "    ordering of the nodes in the graph.\n",
    "    '''\n",
    "    if not args:\n",
    "        args=node_attrs(g)\n",
    "\n",
    "    node_data={}\n",
    "\n",
    "    [sidis.give(node_data,str(arg),\n",
    "                np.squeeze(np.array([sidis.get(g.nodes[n],arg) for n in g.nodes])))\n",
    "        for arg in args]\n",
    "\n",
    "    return node_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-infection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': array([None, None, 2], dtype=object),\n",
       " 'b': array([1, None, None], dtype=object),\n",
       " 'a': array([0.83597281, 0.58115496, 0.00429846])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_data(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def edge_data(g,*args):\n",
    "    '''\n",
    "    Return edge attributes 'args' as an array.\n",
    "    NOTE: The ordering of the array corresponds to the\n",
    "    ordering of the edges in the graph.\n",
    "    '''\n",
    "    if not args:\n",
    "        args=edge_attrs(g)\n",
    "\n",
    "    edge_data={}\n",
    "\n",
    "    [sidis.give(edge_data,str(arg), np.array([sidis.get(g.edges[e],arg) for e in g.edges]))\n",
    "        for arg in args]\n",
    "\n",
    "    return edge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-scout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d': array([0, 1, 2, 3, 4, 5, 6, 7, 8])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_data(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def argwhere(*args : List[np.ndarray]):\n",
    "    '''\n",
    "    Simplified version of np.argwhere for multiple arrays.\n",
    "    Returns list of indices where args hold.\n",
    "    '''\n",
    "    with warnings.catch_warnings(): #ignore numpy warning\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if not args:\n",
    "            return None\n",
    "        elif len(args)==1:\n",
    "            return list(np.ravel(np.argwhere(args[0])).astype(int))\n",
    "        else:\n",
    "            i=[] #indices\n",
    "            for arg in args:\n",
    "                res=list(np.ravel(np.argwhere(arg)).astype(int))\n",
    "                i+=[res]\n",
    "            if len(i)==1:\n",
    "                i=i[0]\n",
    "            if np.any(i):\n",
    "                return list(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-asthma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1], [2], []]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([0,1,2])\n",
    "A=argwhere(a==0,a==1,a>1,a==10)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-fruit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for i in A:\n",
    "    print(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def kwargwhere(g : nx.MultiDiGraph,**kwargs : Dict[str,Any]):\n",
    "    '''\n",
    "    Return the node and edges where\n",
    "    the kwarg equalities hold in the graph.\n",
    "    '''\n",
    "    node_k=node_attrs(g)\n",
    "    edge_k=edge_attrs(g)\n",
    "    node_i=[]\n",
    "    edge_i=[]\n",
    "    for k,v in kwargs.items():\n",
    "        n_i=[]\n",
    "        e_i=[]\n",
    "        if k in node_k:\n",
    "            for n in g.nodes:\n",
    "                if g.nodes[n].get(k)==v:\n",
    "                    n_i+=[n]\n",
    "            node_i+=[n_i]\n",
    "        if k in edge_k:\n",
    "            for e in g.edges:\n",
    "                if g.edges[e].get(k)==v:\n",
    "                    e_i+=[e]\n",
    "            edge_i+=[e_i]\n",
    "\n",
    "    if len(node_i)==1:\n",
    "        node_i=node_i[0]\n",
    "    if len(edge_i)==1:\n",
    "        edge_i=edge_i[0]\n",
    "    if node_i and edge_i:\n",
    "        return node_i,edge_i\n",
    "    elif node_i:\n",
    "        return node_i\n",
    "    elif edge_i:\n",
    "        return edge_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-appointment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, {'a': array([0.83597281]), 'b': 1}), (1, {'a': array([0.58115496])}), (2, {'a': array([0.00429846]), 'c': 2})]\n"
     ]
    }
   ],
   "source": [
    "print(g.nodes(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-scope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargwhere(g,b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-boating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargwhere(g,d=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-advisory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2, {'d': 0}), (0, 1, {'d': 1}), (0, 0, {'d': 2, 'b': 1}), (1, 0, {'d': 3}), (1, 2, {'d': 4}), (1, 1, {'d': 5}), (2, 1, {'d': 6}), (2, 0, {'d': 7}), (2, 2, {'d': 8})]\n"
     ]
    }
   ],
   "source": [
    "g.edges[(0,0,0)]['b']=1\n",
    "print(g.edges(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-feelings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0], [(0, 0, 0)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargwhere(g,b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def where(g,*args,**kwargs):\n",
    "    '''\n",
    "    Combine the 'argwhere' and 'kwargwhere' functions for the graph.\n",
    "    '''\n",
    "    arg_i=argwhere(*args)\n",
    "    kwarg_i=kwargwhere(g,**kwargs)\n",
    "    if arg_i and kwarg_i:\n",
    "        return arg_i,kwarg_i\n",
    "    elif arg_i:\n",
    "        return arg_i\n",
    "    elif kwarg_i:\n",
    "        return kwarg_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-ghana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1], ([0], [(0, 0, 0)]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "where(g,node_data(g)['a']>0.5,b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def parse_lengths(g : nx.MultiDiGraph,\n",
    "                  edges : Union[tuple,List[tuple]],\n",
    "                  lengths : Union[str,int,List[int]] = 1) -> Union[list,List[list]]:\n",
    "    '''\n",
    "    Convert `lengths` corresponding to attributes of each edge into a list of lists.\n",
    "    `lengths` can be a single integer, an integer for each edge, or a string\n",
    "    giving the edge attribute holding the length.\n",
    "    '''\n",
    "    if type(lengths) is int:\n",
    "        lengths={e:lengths for e in edges}\n",
    "    elif type(lengths) is str:\n",
    "        lengths={e:g.edges[e].get(lengths) for e in edges}\n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def convert_edges(g : nx.MultiDiGraph,\n",
    "                  edges : Union[None,tuple,List[tuple]] = None,\n",
    "                  lengths : Union[str,int,dict] = 1,\n",
    "                  node_data : dict = {},\n",
    "                  label : callable = lambda g,node,iterable : len(g)+iterable,\n",
    "                  **edge_data\n",
    "                 ):\n",
    "    '''\n",
    "    Converts `edges` in `g` to paths of the given `lengths`. \n",
    "    The new paths follow a tree structure, and each new node\n",
    "    inherits `node_data` and is labeled with `label`.\n",
    "    The tree structure finds the roots (set of starting nodes)\n",
    "    in the list of `edges`, and then creates trunks corresponding\n",
    "    to the paths of maximum length for each node. Then, branches are\n",
    "    added from the trunk to each of the leaves (terminal nodes), \n",
    "    made from new nodes equal to the lengths associated with each path. \n",
    "    '''\n",
    "    \n",
    "    #default to all edges\n",
    "    if edges is None:\n",
    "        edges=g.edges\n",
    "        \n",
    "    #parse args\n",
    "    edges=parse_edges(edges=g.edges(keys=True),default_key=0)\n",
    "    lengths=parse_lengths(g=g,edges=edges,lengths=lengths)\n",
    "\n",
    "    #unique first nodes\n",
    "    roots=set([e[0] for e in edges])\n",
    "    \n",
    "    #max path lengths on a per-starting node basis\n",
    "    trunks={r:max([lengths[e] for e in g.out_edges(r,keys=True) if e in edges]) \n",
    "            for r in roots} \n",
    "    \n",
    "    #sort roots by longest trunk length to create largest trunks first\n",
    "    roots=sorted(roots,\n",
    "                 key=lambda r: trunks[r],\n",
    "                 reverse=True) \n",
    "    \n",
    "    #terminal nodes for each branch\n",
    "    leaves={r:list(g.successors(r)) for r in roots} \n",
    "\n",
    "    #now build trunks, then create branches from trunk to edges\n",
    "    for r in roots:\n",
    "        trunk=[label(g,node=r,iterable=i) for i in range(trunks[r])]\n",
    "        if trunk!=[]:\n",
    "            nx.add_path(g,[r]+trunk,**parse_kwargs(**edge_data))\n",
    "            give_nodes(g,nodes=trunk,**node_data)\n",
    "            for edge,length in lengths.items():\n",
    "                if edge[0]==r: #branch from root\n",
    "                    \n",
    "                    if length==trunks[r]: #go to leaf using trunk endpoint\n",
    "                        branch=[trunk[-1]]+[edge[1]] \n",
    "                        \n",
    "                    else: #create new branch from somewhere in trunk\n",
    "                        branch=[trunk[length-1]]+[edge[1]]\n",
    "                        \n",
    "                    nx.add_path(g,branch,**g.edges[edge]) #apply old edge data\n",
    "                        \n",
    "                    give_nodes(g,nodes=branch[:-1],**node_data)\n",
    "        \n",
    "    #trim old edges\n",
    "    for e in edges:\n",
    "        g.remove_edge(*e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-dallas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Node|Predecessors|Successors|\n",
      "|0   |1           |2         |\n",
      "|1   |2           |0         |\n",
      "|2   |0           |1         |\n",
      "\n",
      "|Node|Predecessors|Successors|\n",
      "|0   |4           |3         |\n",
      "|1   |5           |4         |\n",
      "|2   |3           |5         |\n",
      "|3   |0           |2         |\n",
      "|4   |1           |0         |\n",
      "|5   |2           |1         |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g=ring()\n",
    "print_graph(g)\n",
    "convert_edges(g,lengths=1)\n",
    "print_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-fleece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Node|Predecessors|Successors|\n",
      "|a   |b           |c         |\n",
      "|c   |a, c, d     |b, c      |\n",
      "|b   |b, c        |a, b      |\n",
      "|d   |            |c         |\n",
      "\n",
      "[('a', 'c', {'delay': 0}), ('c', 'b', {'delay': 1}), ('c', 'c', {'delay': 2}), ('b', 'a', {'delay': 3}), ('b', 'b', {'delay': 4}), ('d', 'c', {'delay': 5})]\n"
     ]
    }
   ],
   "source": [
    "g=nx.MultiDiGraph()\n",
    "g.add_edges_from([('a','c'),('b','a'),('b','b'),('c','b'),('c','c'),('d','c')])\n",
    "print_graph(g)\n",
    "give_edges(g,{e:dict(delay=i) for i,e in enumerate(g.edges)})\n",
    "print(g.edges(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-israel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Node|Predecessors|Successors|\n",
      "|a   |b_3         |          |\n",
      "|c   |d_5, c_2    |c_1       |\n",
      "|b   |b_4, c_1    |b_1       |\n",
      "|d   |            |d_1       |\n",
      "|d_1 |d           |d_2       |\n",
      "|d_2 |d_1         |d_3       |\n",
      "|d_3 |d_2         |d_4       |\n",
      "|d_4 |d_3         |d_5       |\n",
      "|d_5 |d_4         |c         |\n",
      "|b_1 |b           |b_2       |\n",
      "|b_2 |b_1         |b_3       |\n",
      "|b_3 |b_2         |b_4, a    |\n",
      "|b_4 |b_3         |b         |\n",
      "|c_1 |c           |c_2, b    |\n",
      "|c_2 |c_1         |c         |\n",
      "\n",
      "[('c', 'c_1', {'delay': 0}), ('b', 'b_1', {'delay': 0}), ('d', 'd_1', {'delay': 0}), ('d_1', 'd_2', {'delay': 0}), ('d_2', 'd_3', {'delay': 0}), ('d_3', 'd_4', {'delay': 0}), ('d_4', 'd_5', {'delay': 0}), ('d_5', 'c', {'delay': 5}), ('b_1', 'b_2', {'delay': 0}), ('b_2', 'b_3', {'delay': 0}), ('b_3', 'b_4', {'delay': 0}), ('b_3', 'a', {'delay': 3}), ('b_4', 'b', {'delay': 4}), ('c_1', 'c_2', {'delay': 0}), ('c_1', 'b', {'delay': 1}), ('c_2', 'c', {'delay': 2})]\n",
      "[('a', {}), ('c', {}), ('b', {}), ('d', {}), ('d_1', {'tau': 1}), ('d_2', {'tau': 1}), ('d_3', {'tau': 1}), ('d_4', {'tau': 1}), ('d_5', {'tau': 1}), ('b_1', {'tau': 1}), ('b_2', {'tau': 1}), ('b_3', {'tau': 1}), ('b_4', {'tau': 1}), ('c_1', {'tau': 1}), ('c_2', {'tau': 1})]\n"
     ]
    }
   ],
   "source": [
    "convert_edges(g,\n",
    "              edges=None,\n",
    "              lengths='delay',\n",
    "              node_data={'tau':1},\n",
    "              label=lambda g,node,iterable : str(node)+'_'+str(iterable+1),\n",
    "              delay=0\n",
    "             )\n",
    "print_graph(g)\n",
    "print(g.edges(data=True))\n",
    "print(g.nodes(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def relabel_graph(g : nx.MultiDiGraph,\n",
    "            mapping : Union[None,callable,dict] = None):\n",
    "    '''\n",
    "    Relabel nodes in place with desired 'mapping', and store the \n",
    "    `mapping` and `inverse_mapping` as attributes of `g`. \n",
    "    Can be called again without args to relabel to the original map,\n",
    "    which switches the `mapping` and `inverse_mapping`.\n",
    "    If `mapping` is None and `g` has no `mapping`, \n",
    "    defaults to replacing nodes with integers.\n",
    "    If `mapping` is None and `g` has a `mapping`, uses that.\n",
    "    Otherwise, `mapping` is a callable or dict keyed with old node labels \n",
    "    as keys and new node labels as values.\n",
    "    '''\n",
    "    if mapping is None:\n",
    "        if not g.__dict__.get('mapping'):\n",
    "            mapping={n:i for i,n in enumerate(g.nodes)}\n",
    "        else:\n",
    "            mapping=g.mapping\n",
    "            \n",
    "    elif callable(mapping):\n",
    "        mapping=mapping(g)\n",
    "    \n",
    "    inverse_mapping={v:k for k,v in mapping.items()}\n",
    "    def relabel_nodes(G, mapping):\n",
    "        H = nx.MultiDiGraph()\n",
    "        H.add_nodes_from(mapping.get(n, n) for n in G)\n",
    "        H._node.update((mapping.get(n, n), d.copy()) for n, d in G.nodes.items())\n",
    "        if G.is_multigraph():\n",
    "            new_edges = [\n",
    "                (mapping.get(n1, n1), mapping.get(n2, n2), k, d.copy())\n",
    "                for (n1, n2, k, d) in G.edges(keys=True, data=True)\n",
    "            ]\n",
    "\n",
    "            # check for conflicting edge-keys\n",
    "            undirected = not G.is_directed()\n",
    "            seen_edges = set()\n",
    "            for i, (source, target, key, data) in enumerate(new_edges):\n",
    "                while (source, target, key) in seen_edges:\n",
    "                    if not isinstance(key, (int, float)):\n",
    "                        key = 0\n",
    "                    key += 1\n",
    "                seen_edges.add((source, target, key))\n",
    "                if undirected:\n",
    "                    seen_edges.add((target, source, key))\n",
    "                new_edges[i] = (source, target, key, data)\n",
    "\n",
    "            H.add_edges_from(new_edges)\n",
    "        else:\n",
    "            H.add_edges_from(\n",
    "                (mapping.get(n1, n1), mapping.get(n2, n2), d.copy())\n",
    "                for (n1, n2, d) in G.edges(data=True)\n",
    "            )\n",
    "        H.graph.update(G.graph)\n",
    "        return H\n",
    "    gnew=relabel_nodes(g,mapping)\n",
    "    g.__dict__.update(gnew.__dict__)\n",
    "    g.mapping=inverse_mapping\n",
    "    g.inverse_mapping=mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-complement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Node|Predecessors|Successors|\n",
      "|0   |11          |          |\n",
      "|1   |8, 14       |13        |\n",
      "|2   |12, 13      |9         |\n",
      "|3   |            |4         |\n",
      "|4   |3           |5         |\n",
      "|5   |4           |6         |\n",
      "|6   |5           |7         |\n",
      "|7   |6           |8         |\n",
      "|8   |7           |1         |\n",
      "|9   |2           |10        |\n",
      "|10  |9           |11        |\n",
      "|11  |10          |12, 0     |\n",
      "|12  |11          |2         |\n",
      "|13  |1           |14, 2     |\n",
      "|14  |13          |1         |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relabel_graph(g)\n",
    "print_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-beads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Node|Predecessors|Successors|\n",
      "|a   |b_3         |          |\n",
      "|c   |d_5, c_2    |c_1       |\n",
      "|b   |b_4, c_1    |b_1       |\n",
      "|d   |            |d_1       |\n",
      "|d_1 |d           |d_2       |\n",
      "|d_2 |d_1         |d_3       |\n",
      "|d_3 |d_2         |d_4       |\n",
      "|d_4 |d_3         |d_5       |\n",
      "|d_5 |d_4         |c         |\n",
      "|b_1 |b           |b_2       |\n",
      "|b_2 |b_1         |b_3       |\n",
      "|b_3 |b_2         |b_4, a    |\n",
      "|b_4 |b_3         |b         |\n",
      "|c_1 |c           |c_2, b    |\n",
      "|c_2 |c_1         |c         |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relabel_graph(g)\n",
    "print_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sort_graph(g : nx.MultiDiGraph,\n",
    "               nodes_by='in_degree', #g.in_degree, #sorting this function over nodes\n",
    "               node_key=lambda t:sidis.get(t,-1,-1), #last element of sorting tuple\n",
    "               node_args=(), #not accessing any attributes by default\n",
    "               nodes_ascending=True,\n",
    "               edges_by=None, #not generating function evals to sort\n",
    "               edge_key=None,#orders edges, defaults to linear comb of node sort\n",
    "               edge_args=(), #not accessing any edge attrs by default\n",
    "               edges_ascending=False,\n",
    "               relabel=False #relabel to integers\n",
    "              ) -> None:\n",
    "    '''\n",
    "    Sort the graph in place by changing node and edge order.\n",
    "    See `sidis.sort` documentation for explanation of by, key, and args.\n",
    "    Default behavior is to sort nodes by in-degree, and edges by increasing node label,\n",
    "    after relabling nodes to integers. Stores result in 'sorting' attribute.\n",
    "    '''\n",
    "    #parse args; get node sorting attr if str\n",
    "    if type(nodes_by) is str:\n",
    "        nodes_by=sidis.get(g,nodes_by)\n",
    "    #if no edge key given default to ordering by linear comb of node func\n",
    "    if edge_key is None:\n",
    "        edge_key=lambda t:100*nodes_by(t[0])-10*nodes_by(t[1])\n",
    "    \n",
    "    #sort nodes\n",
    "    node_sorting=sidis.sort(g.nodes,\n",
    "                            *node_args,\n",
    "                            by=nodes_by,\n",
    "                            key=node_key,\n",
    "                            reverse=nodes_ascending)\n",
    "\n",
    "    #sort returns tuples of (node,nodes_by(node)), so extract nodes and data\n",
    "    if nodes_by is None:\n",
    "        nodes=[(n,g.nodes[n]) for n in node_sorting]\n",
    "    else:\n",
    "        nodes=[(n[0],g.nodes[n[0]]) for n in node_sorting]  \n",
    "    \n",
    "    #sort edges\n",
    "    edge_sorting=sidis.sort(list(g.edges(keys=True)),\n",
    "                            *edge_args,\n",
    "                            by=edges_by,\n",
    "                            key=edge_key,\n",
    "                            reverse=edges_ascending)\n",
    "    \n",
    "    #extract edge,data tuple\n",
    "    if edges_by is None:\n",
    "        edges=[(*e,g.edges[e]) for e in edge_sorting]\n",
    "    else:\n",
    "        edges=[(*e[0],g.edges[e[0]]) for e in edge_sorting]        \n",
    "    \n",
    "    #wipe graph and add new nodes/edges in order\n",
    "    g.clear()\n",
    "    g.add_nodes_from(nodes)\n",
    "    g.add_edges_from(edges)\n",
    "    \n",
    "    #relabel to new ranking if desired\n",
    "    if relabel:\n",
    "        mapping={n:i for i,n in enumerate([node[0] for node in nodes])}\n",
    "        relabel_graph(g,mapping)\n",
    "        new_node_sorting=[]\n",
    "        for node,rank in node_sorting:\n",
    "            new_node_sorting+=[(g.inverse_mapping[node],rank)]\n",
    "        node_sorting=new_node_sorting\n",
    "    \n",
    "    sorting=nx.utils.groups(dict(node_sorting))\n",
    "    g.sorting={k:list(v) for k,v in sorting.items()}             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-ethernet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "|Node|Predecessors|Successors|\n",
      "|a   |b           |c         |\n",
      "|c   |a, c, d     |b, c      |\n",
      "|b   |b, c        |a, b      |\n",
      "|d   |            |c         |\n",
      "\n",
      "After sorting and relabeling\n",
      "|Node|Predecessors|Successors|\n",
      "|0   |0, 2, 3     |0, 1      |\n",
      "|1   |0, 1        |1, 2      |\n",
      "|2   |1           |0         |\n",
      "|3   |            |0         |\n",
      "\n",
      "Sorting result: g.sorting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3: [0], 2: [1], 1: [2], 0: [3]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g=nx.MultiDiGraph()\n",
    "g.add_edges_from([('a','c'),('b','a'),('b','b'),('c','b'),('c','c'),('d','c')])\n",
    "print('Before')\n",
    "print_graph(g)\n",
    "print('After sorting and relabeling')\n",
    "sort_graph(g,relabel=True)\n",
    "print_graph(g)\n",
    "print('Sorting result: g.sorting')\n",
    "g.sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-toddler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_graph_functions.ipynb.\n",
      "Converted 01_model_functions.ipynb.\n",
      "Converted 02_network_class.ipynb.\n",
      "No export destination, ignored:\n",
      "#export\n",
      "import warnings\n",
      "with warnings.catch_warnings(): #ignore warnings\n",
      "    warnings.simplefilter(\"ignore\")\n",
      "    import networkx as nx\n",
      "    import numpy as np\n",
      "    import sidis\n",
      "    rng=sidis.RNG(0)\n",
      "    import matplotlib.pyplot as plt\n",
      "    import typing\n",
      "    from typing import Optional, Tuple, Dict, Callable, Union, Mapping, Sequence, Iterable, Hashable, List, Any\n",
      "    from collections import namedtuple\n",
      "    import einops\n",
      "    import numba\n",
      "    from numba import njit\n",
      "    from scipy import stats\n",
      "    import scipy.optimize as sciopt\n",
      "    \n",
      "    from networkm.graph_functions import *\n",
      "    from networkm.model_functions import *\n",
      "    #from networkm.network_class import *\n",
      "No export destination, ignored:\n",
      "#export\n",
      "@njit\n",
      "def query_rise(iterator_matrix,\n",
      "               time_delay_matrix,\n",
      "               sigmoid_constant_matrix,\n",
      "               time_constant_matrix,\n",
      "               predecessor_matrix,\n",
      "               initial_condition_matrix,\n",
      "               hold_time_matrix,\n",
      "               dt,\n",
      "               T,\n",
      "               N,\n",
      "               noise_scale,\n",
      "               decimation\n",
      "              ):\n",
      "    '''\n",
      "    Jit accelerated integration routine for the `query` function for an `ensemble` of\n",
      "    `BooleanNetwork`s which have the same rise and fall time. Nearly identical to \n",
      "    `bool_integral` with a loop over `classes,instances,challenges,repeats`. \n",
      "    '''\n",
      "    classes=iterator_matrix.shape[0]\n",
      "    challenges=initial_condition_matrix.shape[0]\n",
      "    repeats=initial_condition_matrix.shape[1]\n",
      "    instances=time_delay_matrix.shape[1]\n",
      "\n",
      "    responses = np.zeros((classes,instances,challenges,repeats,int(T/decimation),N))\n",
      "\n",
      "    for c,q,h,r in np.ndindex(responses.shape[:-2]): #class,inst,chal,rep\n",
      "        iterator=iterator_matrix[c]\n",
      "        time_delays=time_delay_matrix[c,q]\n",
      "        sigmoid_constants=sigmoid_constant_matrix[c,q]\n",
      "        time_constants=time_constant_matrix[c,q]\n",
      "        predecessors=predecessor_matrix[c]\n",
      "        initial_conditions=initial_condition_matrix[h,r]\n",
      "        hold_times=hold_time_matrix[c]   \n",
      "        x=np.zeros((T,N)).astype(np.longdouble)\n",
      "        dx=np.zeros(x.shape[-1]).astype(np.longdouble)\n",
      "        for t in range(x.shape[0]-1):\n",
      "            noise=np.empty(x.shape[1])\n",
      "            for n in range(x.shape[1]):\n",
      "                noise[n]=np.random.random()*noise_scale\n",
      "            #noise=noise*noise_scale\n",
      "            edge_index=0\n",
      "            if t<max(hold_times):\n",
      "                for n in range(x.shape[-1]):\n",
      "                    if hold_times[n]>=t:\n",
      "                        x[t,n]=initial_conditions[n]\n",
      "            for i in range(len(iterator)): \n",
      "                n1,n2,deg,mask=iterator[i]\n",
      "                d=-time_delays[edge_index:edge_index+(n2-n1)*deg].reshape((n2-n1,deg))\n",
      "                d+=t\n",
      "                p=predecessors[edge_index:edge_index+(n2-n1)*deg].reshape((n2-n1,deg))\n",
      "                a=sigmoid_constants[n1:n2].reshape((n2-n1,1))\n",
      "                edge_index+=(n2-n1)*deg\n",
      "                y=np.zeros((n2-n1,deg))#.astype(np.longdouble)\n",
      "                for k in range(n2-n1):\n",
      "                    for j in range(deg):\n",
      "                        de=d[k,j]\n",
      "                        pr=p[k,j]\n",
      "                        y[k,j]=x[de,pr]\n",
      "                y=sigmoid(x=y,a=a)\n",
      "                dx[n1:n2]=BOOL(y,mask)\n",
      "            dx=sigmoid(dx,sigmoid_constants)\n",
      "            dxdt=(-x[t]+dx+noise)/time_constants\n",
      "            x[t+1]=x[t]+dt*dxdt\n",
      "        \n",
      "        responses[c,q,h,r]=x[::decimation]\n",
      "                \n",
      "    return responses\n",
      "No export destination, ignored:\n",
      "#export\n",
      "@njit\n",
      "def njintegral(iterator,\n",
      "               time_delays,\n",
      "               sigmoid_constants,\n",
      "               time_constants,\n",
      "               predecessors,\n",
      "               initial_condition_matrix,\n",
      "               hold_times,\n",
      "               dt,\n",
      "               T,\n",
      "               noise_scale,\n",
      "               decimation,\n",
      "               repeats=1\n",
      "              ):\n",
      "    '''\n",
      "    Jit accelerated integration routine for the `query` function for an `ensemble` of\n",
      "    `BooleanNetwork`s which have the same rise and fall time. Nearly identical to \n",
      "    `bool_integral` with a loop over `classes,instances,challenges,repeats`. \n",
      "    '''\n",
      "    C,N=initial_condition_matrix.shape #challenges,repeats,nodes\n",
      "    responses = np.zeros((C,R,int(T/decimation),N))\n",
      "\n",
      "    for c,r in np.ndindex(C,R): #diff inits\n",
      "        initial_conditions=initial_condition_matrix[c]\n",
      "        x=np.zeros((T,N)).astype(np.longdouble)\n",
      "        dx=np.zeros(x.shape[-1]).astype(np.longdouble)\n",
      "        for t in range(x.shape[0]-1):\n",
      "            noise=np.empty(x.shape[1])\n",
      "            for n in range(x.shape[1]):\n",
      "                noise[n]=np.random.random()*noise_scale\n",
      "            #noise=noise*noise_scale\n",
      "            edge_index=0\n",
      "            if t<max(hold_times):\n",
      "                for n in range(x.shape[-1]):\n",
      "                    if hold_times[n]>=t:\n",
      "                        x[t,n]=initial_conditions[n]\n",
      "            for i in range(len(iterator)): \n",
      "                n1,n2,deg,mask=iterator[i]\n",
      "                d=-time_delays[edge_index:edge_index+(n2-n1)*deg].reshape((n2-n1,deg))\n",
      "                d+=t\n",
      "                p=predecessors[edge_index:edge_index+(n2-n1)*deg].reshape((n2-n1,deg))\n",
      "                a=sigmoid_constants[n1:n2].reshape((n2-n1,1))\n",
      "                edge_index+=(n2-n1)*deg\n",
      "                y=np.zeros((n2-n1,deg))#.astype(np.longdouble)\n",
      "                for k in range(n2-n1):\n",
      "                    for j in range(deg):\n",
      "                        de=d[k,j]\n",
      "                        pr=p[k,j]\n",
      "                        y[k,j]=x[de,pr]\n",
      "                y=sigmoid(x=y,a=a)\n",
      "                dx[n1:n2]=BOOL(y,mask)\n",
      "            dx=sigmoid(dx,sigmoid_constants)\n",
      "            dxdt=(-x[t]+dx+noise)/time_constants\n",
      "            x[t+1]=x[t]+dt*dxdt\n",
      "        \n",
      "        responses[c,r]=x[::decimation]\n",
      "                \n",
      "    return responses\n",
      "No export destination, ignored:\n",
      "#export\n",
      "class BooleanNetwork(nx.MultiDiGraph):\n",
      "    '''\n",
      "    Model the dynamics of the graph `g` by giving the node attributes\n",
      "\n",
      "        f : logical function\n",
      "\n",
      "        a : sigmoid function\n",
      "\n",
      "        tau : time constant\n",
      "\n",
      "    and edge attributes\n",
      "\n",
      "        delay : time-delay\n",
      "\n",
      "    and parses each of these arguments if given as a tuple of a \n",
      "\n",
      "    randomization function and its args; see `parse_kwargs`.\n",
      "\n",
      "    Converts any edges with the given `edge_replacements` (see \n",
      "\n",
      "    `convert_edges` function for arguments); useful for `MPX`. \n",
      "\n",
      "    Sorts the graph in place using `sort_graph` in order to produce\n",
      "\n",
      "    an iterable with `bool_model_iter`. See also `setup_bool_integral`.\n",
      "\n",
      "    Initializes the dynamics to `init` using the hold times `hold`;\n",
      "\n",
      "    see `bool_initial_conditions`. Integrates with `bool_integral`\n",
      "\n",
      "    and `bool_integral_risefall` if `tau` is given as an array of\n",
      "\n",
      "    [rise_time,fall_time]. Returns the node state array `x` \n",
      "\n",
      "    and optionally plots the resulting dynamics with `plot_graph`.\n",
      "\n",
      "    Includes most functions from the `graph_functions` library.\n",
      "    '''\n",
      "    \n",
      "    def edge_replacements(\n",
      "        lengths = 1,\n",
      "        delay = 0,\n",
      "        a = np.inf,\n",
      "        tau = 1,\n",
      "        f = MPX,\n",
      "        label = lambda g,node,iterable : len(g)+iterable\n",
      "    ):\n",
      "        return dict(lengths=lengths,delay=delay,label=label,node_attrs=dict(a=a,f=f,tau=tau))\n",
      "        \n",
      "    mpx_edges = edge_replacements(lengths=1,delay=0,a=np.inf,tau=1,f=MPX)\n",
      "    \n",
      "    delay_edges = edge_replacements(f=COPY,lengths='delay',a=np.inf,tau=1,delay=0)\n",
      "    \n",
      "    def ring(N=3,left=True,right=False,loop=False,\n",
      "             a=np.inf,tau=1,delay=0,f=NOT,edge_replacements=None,\n",
      "             dt=0.01,T=15,noise=0,init=None,hold=None,steady=False,plot=False,view='out'):\n",
      "        g=ring(N=N,left=left,right=right,loop=loop)\n",
      "        return BooleanNetwork(g=g,a=a,tau=tau,f=f,delay=delay,\n",
      "                    edge_replacements=edge_replacements,\n",
      "                   T=T,dt=dt,noise=noise,init=init,hold=hold,steady=steady,plot=plot,view=view)\n",
      "    \n",
      "    def random(N=16,k=3,a=np.inf,tau=1,delay=0,f=NOT,edge_replacements=None,\n",
      "             dt=0.01,T=15,noise=0,init=None,hold=None,steady=False,plot=False,view='out'):\n",
      "        g=nx.random_regular_graph(k,N)\n",
      "        return BooleanNetwork(g=g,a=a,tau=tau,f=f,delay=delay,\n",
      "                    edge_replacements=edge_replacements,\n",
      "                   T=T,dt=dt,noise=noise,init=init,hold=hold,steady=steady,plot=plot,view=view)\n",
      "    '''policy: values in the graph object will be left untouched,\n",
      "               and self attributes will be arrays of functional\n",
      "               transforms from these values, automatically parsed when needed.\n",
      "               Ex: changing .nodes[0]['data'] should be reflected in a\n",
      "               .data attribute after calling .func(data).'''\n",
      "    def __init__(self,\n",
      "                 g = ring(N=3,right=True,left=False,loop=False),\n",
      "                 a = np.inf,\n",
      "                 tau = 1,\n",
      "                 f = XOR,\n",
      "                 delay = 0,\n",
      "                 edge_replacements = None,\n",
      "                 T = 10,\n",
      "                 dt = 1,\n",
      "                 noise = 0,\n",
      "                 init = None,\n",
      "                 hold = None,\n",
      "                 steady = False,\n",
      "                 view = 'out',\n",
      "                 plot = False,\n",
      "                 init = None,\n",
      "                 hold = 1\n",
      "                ):\n",
      "        \n",
      "        #super init\n",
      "        self.kwargs={k:v for k,v in locals().items() if k!='self'}\n",
      "        super().__init__(g)\n",
      "        self.view(view)\n",
      "        \n",
      "        self.original_nodes=list(g.nodes)\n",
      "        self.original_edges=list(g.edges)\n",
      "        \n",
      "        if edge_replacements:\n",
      "            self.convert_edges(edge_replacements)\n",
      "            self.new_nodes=[n for n in self.nodes if n not in self.original_nodes]\n",
      "            self.new_edges=[n for n in self.edges if n not in self.original_edges]\n",
      "        else:\n",
      "            self.new_nodes=None\n",
      "            self.new_edges=None\n",
      "            \n",
      "        self.give_nodes(nodes=self.original_nodes,a=a,tau=tau,f=f)\n",
      "        self.give_edges(edges=self.original_edges,delay=delay)\n",
      "        \n",
      "        self.sort()\n",
      "        self.iter,funcs=bool_model_iter(self,return_funcs=True)\n",
      "        \n",
      "        #set aliases for functions as self attrs\n",
      "        for k,v in funcs.items():\n",
      "            setattr(self,k.__name__,v)\n",
      "        \n",
      "        #set nodes to be forced for initial conds\n",
      "        if self.MPX is not None:\n",
      "            self.dynamical_nodes=self.MPX\n",
      "        else:\n",
      "            self.dynamical_nodes=original_nodes\n",
      "        \n",
      "        self.noise=noise\n",
      "        self.dt=dt\n",
      "        self.T=int(T/dt)\n",
      "        self.delay=(self.delay/dt).astype(np.int64)\n",
      "        self.node_data(save=True)\n",
      "        self.edge_data(save=True)\n",
      "        self.init,self.hold=bool_initial_conditions(self,init=init,hold=hold,steady=steady)\n",
      "        self.hold=(self.hold/dt).astype(np.int64)\n",
      "        self.predar=np.concatenate([list(self.predecessors(n)) for n in self.nodes]).astype(np.int64)\n",
      "        if len(self.tau.shape)!=1:\n",
      "            self.integral=bool_integral_risefall\n",
      "        else:\n",
      "            self.integral=bool_integral\n",
      "        \n",
      "        \n",
      "        #self(g=g,a=a,tau=tau,f=f,delay=delay,edge_replacements=edge_replacements,\n",
      "        #           T=T,dt=dt,noise=noise,steady=steady,init=init,hold=hold,plot=plot)\n",
      "    \n",
      "    \n",
      "    def __call__(self,\n",
      "                 g=None,\n",
      "                 a=None,\n",
      "                 tau=None,\n",
      "                 f=None,\n",
      "                 delay=None,\n",
      "                 edge_replacements=None,\n",
      "                 T=None,\n",
      "                 dt=None,\n",
      "                 noise=None,\n",
      "                 steady=None,\n",
      "                 init=None,\n",
      "                 hold=None,\n",
      "                 plot=None):\n",
      "        if g is None:\n",
      "            g = self.kwargs['g']\n",
      "        if a is None: \n",
      "            a = self.kwargs['a']\n",
      "        if tau is None: \n",
      "            tau = self.kwargs['tau']\n",
      "        if f is None: \n",
      "            f = self.kwargs['f']\n",
      "        if delay is None: \n",
      "            delay = self.kwargs['delay']\n",
      "        if edge_replacements is None: \n",
      "            edge_replacements = self.kwargs['edge_replacements']\n",
      "        if T is None: \n",
      "            T = self.kwargs['T']\n",
      "        if dt is None: \n",
      "            dt = self.kwargs['dt']\n",
      "        if noise is None: \n",
      "            noise = self.kwargs['noise']\n",
      "        if steady is None: \n",
      "            steady = self.kwargs['steady']\n",
      "        if init is None: \n",
      "            init = self.kwargs['init']\n",
      "        if hold is None: \n",
      "            hold = self.kwargs['hold']\n",
      "        if plot is None: \n",
      "            plot = self.kwargs['plot']\n",
      "        super().__init__(g)\n",
      "        self.give_nodes(a=a,tau=tau,f=f)\n",
      "        self.give_edges(delay=delay)\n",
      "        original_nodes=list(self.nodes)\n",
      "        if edge_replacements:\n",
      "            convert_edges(self,edges=edge_replacements.get('edges'),\n",
      "                               lengths=edge_replacements.get('lengths'),\n",
      "                               node_data=edge_replacements.get('node_attrs'),\n",
      "                               delay=edge_replacements.get('delay'))\n",
      "        self.sort()\n",
      "        self.itr=bool_model_iter(self)\n",
      "        node_funcs={n:self.nodes[n]['f'] for n in self.nodes}\n",
      "        funcs=nx.utils.groups(sidis.cast(node_funcs,dict))\n",
      "        funcs={k:list(v) for k,v in funcs.items()}\n",
      "        for k,v in funcs.items():\n",
      "            setattr(self,k.__name__,v)\n",
      "        if self.MPX is not None:\n",
      "            self.dynamical_nodes=self.MPX\n",
      "        else:\n",
      "            self.dynamical_nodes=original_nodes\n",
      "        self.dt=dt\n",
      "        self.T=int(T/dt)\n",
      "        self.node_data(save=True)\n",
      "        self.edge_data(save=True)\n",
      "        self.init,self.hold=bool_initial_conditions(self,init=init,hold=hold,steady=steady)\n",
      "        self.hold=(self.hold/dt).astype(np.int64)\n",
      "        self.delay=(self.delay/dt).astype(np.int64)\n",
      "        self.predar=np.concatenate([list(self.predecessors(n)) for n in self.nodes]).astype(np.int64)\n",
      "        if len(self.tau.shape)!=1:\n",
      "            self.integral=njintegral\n",
      "        else:\n",
      "            self.integral=njintegral\n",
      "        if plot:\n",
      "            self.integrate(init=self.init,hold=self.hold)\n",
      "            self.plot()\n",
      "\n",
      "    def initial_condition(self,init=None,steady=False,bias=0):\n",
      "        if init is None:\n",
      "            init=np.zeros((len(self.dynamical_nodes)))\n",
      "            init[0]=1\n",
      "        init=np.array(init)\n",
      "        new_init=np.zeros((len(self)))\n",
      "        new_init[self.dynamical_nodes]=init[:len(self.dynamical_nodes)]\n",
      "        if steady:\n",
      "            new_init=np.array([self.nodes[n]['f'](\n",
      "                np.array([new_init[j] for j in list(self.predecessors(n))])) for n in self.nodes])\n",
      "        new_init+=bias\n",
      "        new_init=np.clip(new_init,0,1)\n",
      "        return new_init\n",
      "    \n",
      "    def parse_initial_condition(self,init=None,hold=None,steady=False,override=False):\n",
      "        if not override:\n",
      "            if init is not None and hold is not None: #if both given, replace current\n",
      "                self.init,self.hold=bool_initial_conditions(self,init=init,hold=hold,steady=steady)\n",
      "                self.hold=(self.hold/dt).astype(np.int64)\n",
      "            if init is None: #replace hold\n",
      "                _,self.hold=bool_initial_conditions(self,init=init,hold=hold,steady=steady)\n",
      "                self.hold=(self.hold/dt).astype(np.int64)\n",
      "            if hold is None: #replace init\n",
      "                self.init,_=bool_initial_conditions(self,init=init,hold=hold,steady=steady)\n",
      "        else:\n",
      "            self.init,self.hold=init,hold\n",
      "\n",
      "    def perturb(self,*args,scale=0.1):\n",
      "        for a in args:\n",
      "            setattr(self,a,perturb(getattr(self,a),scale))\n",
      "        \n",
      "    def integrate(self,init=None,hold=None,noise=None,T=None,dt=None,steady=False,override=False,\n",
      "                  repeats=1,decimation=1):\n",
      "        if decimation is None:\n",
      "            decimation=int(1/dt)\n",
      "        if T is None:\n",
      "            T=self.T\n",
      "        if dt is not None:\n",
      "            self.dt=dt\n",
      "        if T is not None:\n",
      "            self.T=int(T/self.dt)\n",
      "        if noise is not None:\n",
      "            self.noise=noise\n",
      "\n",
      "        if isinstance(init,np.ndarray) and len(init.shape)>1:\n",
      "            inits=[]\n",
      "            for i in init:\n",
      "                self.parse_initial_condition(init=i,hold=hold,steady=steady,override=override)\n",
      "                inits+=[self.init]\n",
      "            self.init=np.array(init)\n",
      "        else:\n",
      "            self.parse_initial_condition(init=init,hold=hold,steady=steady,override=override)\n",
      "        \n",
      "        x=self.integral(iterator=self.iter,\n",
      "                        time_delays=self.delay,\n",
      "                        sigmoid_constants=self.a,\n",
      "                        time_constants=self.tau,\n",
      "                        predecessors=self.predar,\n",
      "                        initial_condition_matrix=self.init if len(self.init.shape)==2 else stack(\n",
      "                            self.init,times=1,axis=0),\n",
      "                        hold_times=self.hold,\n",
      "                        dt=self.dt,\n",
      "                        T=self.T,\n",
      "                        noise_scale=self.noise,\n",
      "                        decimation=decimation,\n",
      "                        repeats=repeats\n",
      "                       )\n",
      "\n",
      "        return np.squeeze(x)\n",
      "    \n",
      "    def view(self,view='in'):\n",
      "        '''\n",
      "        Change default edge view\n",
      "        '''\n",
      "        if view=='in':\n",
      "            self.edge_view=nx.classes.reportviews.InMultiEdgeView\n",
      "\n",
      "        elif view=='out':\n",
      "            self.edge_view=nx.classes.reportviews.OutMultiEdgeView\n",
      "            \n",
      "    \n",
      "    @nx.MultiDiGraph.edges.getter\n",
      "    def edges(self):\n",
      "        return self.edge_view(self)\n",
      "    \n",
      "    \n",
      "    def sort(self,**kwargs):\n",
      "        sort_graph(self,**kwargs)\n",
      "        \n",
      "\n",
      "    def relabel(self,mapping=None):\n",
      "        relabel_graph(self,mapping)\n",
      "        \n",
      "\n",
      "    def node_attrs(self):\n",
      "        return node_attrs(self)\n",
      "    \n",
      "\n",
      "    def edge_attrs(self):\n",
      "        return edge_attrs(self)\n",
      "    \n",
      "\n",
      "    def give_nodes(self,data=None,nodes=None,**kwargs):\n",
      "        give_nodes(self,data=data,nodes=nodes,**kwargs)\n",
      "            \n",
      "\n",
      "    def give_edges(self,data=None,edges=None,**kwargs):\n",
      "        give_edges(self,data=data,edges=edges,**kwargs)\n",
      "\n",
      "    def give_self(self,data=None,**kwargs):\n",
      "        '''\n",
      "        Give any arg dict and kwargs to self as attrs\n",
      "        '''\n",
      "        try:\n",
      "            sidis.give(self,**parse_kwargs(**kwargs))\n",
      "        except:\n",
      "            pass\n",
      "\n",
      "        if data:\n",
      "            for k,v in data.items():\n",
      "                try:\n",
      "                    self.__dict__.update(parse_kwargs(**v))\n",
      "                except KeyError:\n",
      "                    pass\n",
      "            \n",
      "            \n",
      "    def clear_nodes(self,*args):\n",
      "        '''\n",
      "        Remove arg entries from node dict\n",
      "        '''\n",
      "        if not args:\n",
      "            args=self.node_attrs()\n",
      "        [[sidis.give(self.node[n],arg,None) for n in self.nodes] for arg in args]\n",
      "        \n",
      "    \n",
      "    def clear_edges(self,*args):\n",
      "        '''\n",
      "        Remove arg entries from edge dict\n",
      "        '''\n",
      "        if not args:\n",
      "            args=self.edge_attrs()\n",
      "        [[sidis.give(self.edges[e],arg,None) for e in self.edges] for arg in args]\n",
      "        \n",
      "\n",
      "    def node_data(self,*args,save=False):\n",
      "        '''\n",
      "        Set node attributes as self attribute array. \n",
      "        '''\n",
      "        if not save:\n",
      "            return node_data(self,*args)\n",
      "        else:\n",
      "            self.give_self(**node_data(self,*args))\n",
      "\n",
      "\n",
      "    def edge_data(self,*args,save=False):        \n",
      "        if not save:\n",
      "            return edge_data(self,*args)\n",
      "        else:\n",
      "            self.give_self(**edge_data(self,*args))\n",
      "                \n",
      "                    \n",
      "    def replace_edges(self,edge_replacements=None):\n",
      "        if edge_replacements is not None:\n",
      "            edges=edge_replacements.get('edges')\n",
      "            lengths=edge_replacements.get('lengths')\n",
      "            node_data=edge_replacements.get('node_attrs')\n",
      "            a=node_data.get('a')\n",
      "            tau=node_data.get('tau')\n",
      "            f=node_data.get('f')\n",
      "            delay=edge_replacements.get('delay')\n",
      "            if label is None:\n",
      "                label = lambda g,node,iterable : len(g)+iterable\n",
      "            if lengths is None:\n",
      "                lengths=1\n",
      "            if delay is None:\n",
      "                delay = 0\n",
      "            if a is None:\n",
      "                a=np.inf\n",
      "            if tau is None:\n",
      "                tau=1\n",
      "            if f is None:\n",
      "                f=MPX\n",
      "            convert_edges(self,edges=edges,\n",
      "                          **BooleanNetwork.edge_replacements(\n",
      "                              lengths=lengths,a=a,f=f,tau=tau,delay=delay,label=label))\n",
      "        \n",
      "        \n",
      "    def where(self,*args,**kwargs):\n",
      "        return where(self,*args,**kwargs)\n",
      "    \n",
      "    def edgewhere(self,*args):\n",
      "        return np.array(self.edges)[self.where(*args)]\n",
      "    \n",
      "    def nodewhere(self,*args):\n",
      "        return np.array(self.nodes)[self.where(*args)]\n",
      "    \n",
      "    def __str__(self):\n",
      "        return print_graph(self,string=True)\n",
      "    \n",
      "    def __repr__(self):\n",
      "        s=str(self)\n",
      "        S=super().__repr__()\n",
      "        spl=s.split('\\n')\n",
      "        if len(spl)>10:\n",
      "            spl=spl[:5]+['...']+spl[-5:]\n",
      "        return S+'\\n'+('\\n').join(spl)\n",
      "\n",
      "    def plot(self):\n",
      "        '''\n",
      "        Make separate plots of the node states `x` for each in-degree grouping.\n",
      "        '''\n",
      "        self.node_funcs={n:self.nodes[n]['f'] for n in self.nodes}\n",
      "        funcs=nx.utils.groups(sidis.cast(self.node_funcs,dict))\n",
      "        funcs={k:list(v) for k,v in funcs.items()}\n",
      "        for f,nodes in funcs.items():\n",
      "            for i in nodes:\n",
      "                plt.plot(np.arange(self.x.shape[0])*self.dt,self.x[:,i])\n",
      "                title=f'{f.__name__} Nodes: {nodes[0]} to {nodes[-1]}'\n",
      "                plt.title(title)\n",
      "                plt.xlabel('Time (ns)')\n",
      "                plt.ylabel('Amplitude')\n",
      "            plt.show()\n",
      "        \n",
      "No export destination, ignored:\n",
      "#export\n",
      "def puf_statistics(responses : np.ndarray,\n",
      "                   shape : str = 'challenge repeat time instance node'\n",
      "                  ) -> Tuple[Union[np.ndarray,float,int]]:\n",
      "    '''\n",
      "    Given an array of `responses` of a given `shape`, calculate the inter and intra\n",
      "    PUF statistics mu_inter and mu_intra and their standard deviations over time.\n",
      "    Return these as arrays over time, as well as their maximum separation mu_opt\n",
      "    and the time at which this occurs t_opt. \n",
      "    `shape` must be a permutation of the words 'challenge repeat time instance node'\n",
      "    and describes the input shape of the array.\n",
      "    '''\n",
      "    resp=einops.rearrange(responses,shape+\n",
      "                          '-> challenge repeat time instance node').astype(float)\n",
      "    n_distinct=resp.shape[0]\n",
      "    n_repeat=resp.shape[1]\n",
      "    measure_time=resp.shape[2]\n",
      "    n_synth=resp.shape[3]\n",
      "    n=resp.shape[4]\n",
      "    #Number of pairwise combinations for inter and intra calculations.\n",
      "    l_intra=n_repeat*(n_repeat-1)/2\n",
      "    l_inter=n_synth*(n_synth-1)/2\n",
      "    #Pairwise differences of timeseries from responses used for comparison.\n",
      "    pdt_intra=np.zeros((n_distinct,n_synth,int(l_intra),measure_time))\n",
      "    pdt_inter=np.zeros((n_distinct,n_repeat,int(l_inter),measure_time))\n",
      "    #Loop over each pairwise combination and form fractional hamming distances at each time.\n",
      "    for i in range(n_distinct):\n",
      "        for g in range(n_synth):\n",
      "            l=0\n",
      "            for j in range(n_repeat):\n",
      "                for k in range(j+1,n_repeat):\n",
      "                    pdt_intra[i,g,l]=np.sum(abs(resp[i,j,:,g,:]-resp[i,k,:,g,:]),axis=-1)/n\n",
      "                    l+=1\n",
      "        for g in range(n_repeat):\n",
      "            l=0\n",
      "            for j in range(n_synth):\n",
      "                for k in range(j+1,n_synth):\n",
      "                    pdt_inter[i,g,l]=np.sum(abs(resp[i,g,:,j,:]-resp[i,g,:,k,:]),axis=-1)/n\n",
      "                    l+=1\n",
      "    #Compute means on a per-device and overall level.\n",
      "    #Intra block below. Copies along axes for quick vector calculations.\n",
      "    mu_intra_per_device=np.mean(pdt_intra,axis=(0,2))\n",
      "    mu_intra_per_device_copy=np.repeat(\\\n",
      "        np.repeat(mu_intra_per_device[:,np.newaxis,:],\\\n",
      "            l_intra,axis=1)[np.newaxis,:],n_distinct,axis=0)\n",
      "    sigma_intra_per_device= np.sqrt(np.mean((np.square(pdt_intra-mu_intra_per_device_copy)),axis=(0,2)))\n",
      "    mu_intra=np.mean(mu_intra_per_device,axis=0)\n",
      "    sigma_intra=np.mean(sigma_intra_per_device,axis=0)\n",
      "    #Inter block below. Copies along axes for quick vector calculations.\n",
      "    mu_inter_per_device=np.mean(pdt_inter,axis=(0,2))\n",
      "    mu_inter_per_device_copy=np.repeat(np.repeat(\\\n",
      "        mu_inter_per_device[:,np.newaxis,:],l_inter,axis=1)[np.newaxis,:],\\\n",
      "            n_distinct,axis=0)\n",
      "    sigma_inter_per_device= np.sqrt(np.mean((np.square(pdt_inter-mu_inter_per_device_copy)),axis=(0,2)))\n",
      "    mu_inter=np.mean(mu_inter_per_device,axis=0)\n",
      "    sigma_inter=np.mean(sigma_inter_per_device,axis=0)\n",
      "    #Find optimum measurement time and save time series.\n",
      "    t_opt=np.argmin(mu_intra-mu_inter)\n",
      "    mu_opt=mu_inter[t_opt]-mu_intra[t_opt]\n",
      "    return mu_inter,mu_intra,sigma_inter,sigma_intra,mu_opt,t_opt\n",
      "No export destination, ignored:\n",
      "#export\n",
      "def bool_lyapunov(responses : np.ndarray,\n",
      "                  window : int = 5,\n",
      "                  shape : str = 'challenge repeat time node'\n",
      "                 ) -> Tuple[np.ndarray]:\n",
      "    \"\"\"\n",
      "    Boolean timeseries Lyapunov exponent calculator.\n",
      "    Takes as input the time series of PUF responses\n",
      "    and calculates the maximum Lyapunov exponent to determine if the system is chaotic.\n",
      "    Args:\n",
      "        responses: array of PUF responses for a single instance\n",
      "        window: Temporal window length for comparison of Boolean distance between time series.\n",
      "        shape: input shape of PUF responses for a single instance\n",
      "    \"\"\"\n",
      "    T=window\n",
      "    resp=einops.rearrange(responses,shape+\n",
      "                          '-> challenge repeat time node').astype(float)\n",
      "    nrp= resp.shape[1] #number of reps\n",
      "    nch= resp.shape[0] #number of challenges\n",
      "    measure_time=resp.shape[2]\n",
      "    n=resp.shape[-1]\n",
      "    #We loop over pairwise combinations of timeseries comparisons and compute the\n",
      "    #average Boolean distance within the window length.\n",
      "    clist=[]\n",
      "    for c in range(nch):\n",
      "        ilist=[]\n",
      "        for i in range(1,nrp):\n",
      "            tlist=[]\n",
      "            for t in range(measure_time-T):\n",
      "                d=np.sum(abs(resp[c,i,t:t+T]-resp[c,0,t:t+T]))/(n*T)\n",
      "                if d!=0:\n",
      "                    first_t=t\n",
      "                    break\n",
      "                else:\n",
      "                    first_t=0\n",
      "            for t in range(first_t,measure_time-T):\n",
      "                d=np.sum(abs(resp[c,i,t:t+T]-resp[c,0,t:t+T]))/(n*T)\n",
      "                tlist+=[d]\n",
      "            if tlist!=[]:\n",
      "                ilist+=[tlist]\n",
      "        if ilist!=[]:\n",
      "            clist+=[ilist]\n",
      "\n",
      "    avgln=[0 for t in range(measure_time-T)]\n",
      "    tcounterlist=[0 for t in range(measure_time-T)]\n",
      "    longesttime=-1\n",
      "    for c in range(nch):\n",
      "        for i in range(len(clist[c])):\n",
      "            for t in range(len(clist[c][i])):\n",
      "                tcounterlist[t]+=1\n",
      "                avgln[t]+=np.log(clist[c][i][t]) if clist[c][i][t]!=0 else np.log(0.01)\n",
      "                longesttime=t if t>longesttime else longesttime\n",
      "\n",
      "    avgln=avgln[:longesttime+1]\n",
      "    tcounterlist=tcounterlist[:longesttime+1]\n",
      "\n",
      "    for t in range(len(avgln)):\n",
      "        avgln[t]=avgln[t]/tcounterlist[t]\n",
      "\n",
      "    sigmaln=[0 for t in range(len(avgln))]\n",
      "    for c in range(nch):\n",
      "        for i in range(len(clist[c])):\n",
      "            for t in range(len(clist[c][i])):\n",
      "                xi=np.log(clist[c][i][t]) if clist[c][i][t]!=0 else np.log(0.01)\n",
      "                sigmaln[t]+=(xi-avgln[t])**2\n",
      "\n",
      "    for t in range(len(avgln)):\n",
      "        sigmaln[t]=np.sqrt(sigmaln[t]/(tcounterlist[t]-1))\n",
      "    \n",
      "    return avgln,sigmaln\n",
      "No export destination, ignored:\n",
      "#export\n",
      "def lya_fit(avgln,sigmaln,lstart=0,lend=5,intercept=False):\n",
      "    \"\"\"\n",
      "    Fits average logarithm of boolean distances calculated in lya() function.\n",
      "    Calculates resulting maximum lyapunov exponent.\n",
      "    Args:\n",
      "        lstart: Start of linear fit.\n",
      "        lend: End of linear fit.\n",
      "    \"\"\"\n",
      "    linearstart=lstart\n",
      "    linearend=lend\n",
      "    xdata=np.arange(linearstart,linearend+1)\n",
      "    ydata=np.asarray(avgln[linearstart:linearend+1])\n",
      "    sdata=np.asarray(sigmaln[linearstart:linearend+1])\n",
      "    def lin(x,m,b):\n",
      "        return m*x+b\n",
      "    popt, pcov = sciopt.curve_fit(lin, xdata, ydata, sigma=sdata, absolute_sigma=True)\n",
      "    m=popt[0]\n",
      "    b=popt[1]\n",
      "    p_sigma = np.sqrt(np.diag(pcov))\n",
      "    m_sigma = p_sigma[0]\n",
      "    residuals = ydata- lin(xdata, m,b)\n",
      "    ss_res = np.sum(residuals**2)\n",
      "    ss_tot = np.sum((ydata-np.mean(ydata))**2)\n",
      "    r_squared = 1 - (ss_res / ss_tot)\n",
      "    lya_b=b\n",
      "    lya_max=m\n",
      "    lya_max_err=m_sigma\n",
      "    if not intercept:\n",
      "        return lya_max,lya_max_err\n",
      "    else:\n",
      "        return lya_max,lya_max_err,lya_b\n",
      "No export destination, ignored:\n",
      "#export\n",
      "@njit\n",
      "def booleanize(vn, threshold=0.5):\n",
      "    '''\n",
      "    Convert the numpy array `vn` into a bitstream\n",
      "    according to `threshold`; values of `vn>=threshold`\n",
      "    will be set to `1`, and values of `vn<threshold`\n",
      "    will be set to `0`. If `threshold` is not supplied,\n",
      "    it defaults to halfway between the range of `vn`.\n",
      "    '''\n",
      "    if threshold is None:\n",
      "        threshold=(np.max(vn)-np.min(vn))/2\n",
      "    B=np.zeros(vn.shape).astype(vn.dtype)\n",
      "    for s in np.ndindex(vn.shape):\n",
      "        if vn[s]>=threshold:\n",
      "            B[s]+=1.\n",
      "    return B\n",
      "No export destination, ignored:\n",
      "#export\n",
      "def ensemble(classes = 3,\n",
      "             instances = 3,\n",
      "             challenges = 3,\n",
      "             repeats = 3,\n",
      "             inter_variation = 0.1,\n",
      "             T = 15,\n",
      "             dt = 0.01,\n",
      "             hold = (rng.normal,1,0.1),\n",
      "             noise = 0.01,\n",
      "             decimation = None,\n",
      "             g = (nx.random_regular_graph,3,16),\n",
      "             a = (rng.normal,20,2),\n",
      "             tau = (rng.normal,1,0.1),\n",
      "             f = XOR,\n",
      "             delay = (rng.random,0,1),\n",
      "             edge_replacements = dict(\n",
      "                 lengths = 1,\n",
      "                 delay = (rng.random,0,0.5),\n",
      "                 node_attrs = dict(\n",
      "                     a = (rng.normal,20,2),\n",
      "                     tau = (rng.normal,0.5,0.05),\n",
      "                     f = MPX)\n",
      "             ),\n",
      "             return_classes = False\n",
      "            ):\n",
      "    '''\n",
      "    Generates an ensemble of BooleanNetworks class objects over a distribution,\n",
      "    and an additional set of arrays containing slight perturbations to the parameter\n",
      "    space of each BooleanNetwork. Returns a dictionary containing the characteristic \n",
      "    parameters of each BooleanNetwork as arrays, which are used as the arguments to\n",
      "    a jit accelerated integration routine `query_rise` and `query_fall`.\n",
      "    '''\n",
      "    if decimation is None:\n",
      "        decimation=int(1/dt)\n",
      "    perturb = lambda a: (a*(1.+rng.random(-inter_variation/2.,inter_variation/2.,shape=a.shape))\n",
      "                        ).astype(a.dtype)\n",
      "    \n",
      "    models = [None for c in range(classes)]\n",
      "    hold_times = [None for c in range(classes)]\n",
      "    iterators = [None for c in range(classes)]\n",
      "    predecessors = [None for c in range(classes)]\n",
      "    \n",
      "    time_delays = [[None for i in range(instances)] for c in range(classes)]\n",
      "    sigmoid_constants = [[None for i in range(instances)] for c in range(classes)]\n",
      "    time_constants = [[None for i in range(instances)] for c in range(classes)]\n",
      "\n",
      "    for c in range(classes):\n",
      "        graph = parse_kwargs(g=g)['g']\n",
      "        N = len(graph) #assume all graphs same len\n",
      "        hold_args = tuple(list(hold)+[dict(shape=N)])\n",
      "        model = BooleanNetwork(g=graph,\n",
      "                               init=None,\n",
      "                               hold=parse_kwargs(k=hold_args)['k'],\n",
      "                               a=a,\n",
      "                               T=T,\n",
      "                               dt=dt,\n",
      "                               noise=noise,\n",
      "                               f=f,\n",
      "                               tau=tau,\n",
      "                               delay=delay,\n",
      "                               edge_replacements=edge_replacements,\n",
      "                               plot=False)\n",
      "        models[c] = model\n",
      "        hold_times[c] = model.hold\n",
      "        iterators[c] = model.itr\n",
      "        predecessors[c] = model.predar\n",
      "        \n",
      "        for i in range(instances):\n",
      "            time_delays[c][i] = perturb(model.delay)\n",
      "            sigmoid_constants[c][i] = perturb(model.a)\n",
      "            time_constants[c][i] = perturb(model.tau)\n",
      "            \n",
      "    iterators = np.array(iterators)\n",
      "    predecessors = np.array(predecessors)\n",
      "    time_delays =  np.array(time_delays)\n",
      "    sigmoid_constants = np.array(sigmoid_constants)\n",
      "    time_constants = np.array(time_constants)\n",
      "    hold_times = np.array(hold_times)\n",
      "    \n",
      "    initial_conditions = np.empty((challenges,repeats,len(model)))\n",
      "    for h in range(challenges): #assume length of all classes equal \n",
      "        initial_conditions[h,:],_=bool_initial_conditions(model,\n",
      "                    init=rng.random(0,1,absval=True,asint=True,shape=N),\n",
      "                    hold=None,\n",
      "                    steady=True)\n",
      "\n",
      "    int_type = np.int64\n",
      "    float_type = np.longdouble\n",
      "    data = dict(\n",
      "        iterator_matrix=iterators.astype(int_type),\n",
      "        time_delay_matrix=time_delays.astype(int_type),\n",
      "        sigmoid_constant_matrix=sigmoid_constants.astype(float_type),\n",
      "        time_constant_matrix=time_constants.astype(float_type),\n",
      "        predecessor_matrix=predecessors.astype(int_type),\n",
      "        initial_condition_matrix=initial_conditions.astype(float_type),\n",
      "        hold_time_matrix=hold_times.astype(int_type),\n",
      "        dt=dt,\n",
      "        noise_scale=noise,\n",
      "        T=T,\n",
      "        N=len(model),\n",
      "        decimation=decimation\n",
      "    )\n",
      "    if return_classes:\n",
      "        return models,data\n",
      "    else:\n",
      "        return data\n",
      "No export destination, ignored:\n",
      "#export\n",
      "@njit\n",
      "def query_rise(iterator_matrix,\n",
      "               time_delay_matrix,\n",
      "               sigmoid_constant_matrix,\n",
      "               time_constant_matrix,\n",
      "               predecessor_matrix,\n",
      "               initial_condition_matrix,\n",
      "               hold_time_matrix,\n",
      "               dt,\n",
      "               T,\n",
      "               N,\n",
      "               noise_scale,\n",
      "               decimation\n",
      "              ):\n",
      "    '''\n",
      "    Jit accelerated integration routine for the `query` function for an `ensemble` of\n",
      "    `BooleanNetwork`s which have the same rise and fall time. Nearly identical to \n",
      "    `bool_integral` with a loop over `classes,instances,challenges,repeats`. \n",
      "    '''\n",
      "    classes=iterator_matrix.shape[0]\n",
      "    challenges=initial_condition_matrix.shape[0]\n",
      "    repeats=initial_condition_matrix.shape[1]\n",
      "    instances=time_delay_matrix.shape[1]\n",
      "\n",
      "    responses = np.zeros((classes,instances,challenges,repeats,T,N))\n",
      "\n",
      "    for c,q,h,r in np.ndindex(responses.shape[:-2]): #class,inst,chal,rep\n",
      "        iterator=iterator_matrix[c]\n",
      "        time_delays=time_delay_matrix[c,q]\n",
      "        sigmoid_constants=sigmoid_constant_matrix[c,q]\n",
      "        time_constants=time_constant_matrix[c,q]\n",
      "        predecessors=predecessor_matrix[c]\n",
      "        initial_conditions=initial_condition_matrix[h,r]\n",
      "        hold_times=hold_time_matrix[c]   \n",
      "        x=np.zeros((int(T/dt),N)).astype(np.longdouble)\n",
      "        dx=np.zeros(x.shape[-1]).astype(np.longdouble)\n",
      "        for t in range(x.shape[0]-1):\n",
      "            noise=np.empty(x.shape[1])\n",
      "            for n in range(x.shape[1]):\n",
      "                noise[n]=np.random.random()*noise_scale\n",
      "            #noise=noise*noise_scale\n",
      "            edge_index=0\n",
      "            if t<max(hold_times):\n",
      "                for n in range(x.shape[-1]):\n",
      "                    if hold_times[n]>=t:\n",
      "                        x[t,n]=initial_conditions[n]\n",
      "            for i in range(len(iterator)): \n",
      "                n1,n2,deg,mask=iterator[i]\n",
      "                d=-time_delays[edge_index:edge_index+(n2-n1)*deg].reshape((n2-n1,deg))\n",
      "                d+=t\n",
      "                p=predecessors[edge_index:edge_index+(n2-n1)*deg].reshape((n2-n1,deg))\n",
      "                a=sigmoid_constants[n1:n2].reshape((n2-n1,1))\n",
      "                edge_index+=(n2-n1)*deg\n",
      "                y=np.zeros((n2-n1,deg))#.astype(np.longdouble)\n",
      "                for k in range(n2-n1):\n",
      "                    for j in range(deg):\n",
      "                        de=d[k,j]\n",
      "                        pr=p[k,j]\n",
      "                        y[k,j]=x[de,pr]\n",
      "                y=sigmoid(x=y,a=a)\n",
      "                dx[n1:n2]=BOOL(y,mask)\n",
      "            dx=sigmoid(dx,sigmoid_constants)\n",
      "            dxdt=(-x[t]+dx+noise)/time_constants\n",
      "            x[t+1]=x[t]+dt*dxdt\n",
      "        \n",
      "        responses[c,q,h,r]=x[::decimation]\n",
      "                \n",
      "    return responses\n",
      "No export destination, ignored:\n",
      "#export\n",
      "@njit\n",
      "def query_fall(iterator_matrix,\n",
      "               time_delay_matrix,\n",
      "               sigmoid_constant_matrix,\n",
      "               time_constant_matrix,\n",
      "               predecessor_matrix,\n",
      "               initial_condition_matrix,\n",
      "               hold_time_matrix,\n",
      "               dt,\n",
      "               T,\n",
      "               N,\n",
      "               noise_scale,\n",
      "               decimation\n",
      "              ):\n",
      "    '''\n",
      "    Jit accelerated integration routine for the `query` function for an `ensemble` of\n",
      "    `BooleanNetwork`s which have different rise and fall times. Nearly identical to \n",
      "    `bool_integral_risefall` with a loop over `classes,instances,challenges,repeats`. \n",
      "    '''\n",
      "    classes=iterator_matrix.shape[0]\n",
      "    challenges=initial_condition_matrix.shape[0]\n",
      "    repeats=initial_condition_matrix.shape[1]\n",
      "    instances=time_delay_matrix.shape[1]\n",
      "\n",
      "    responses = np.zeros((classes,instances,challenges,repeats,T,N))\n",
      "\n",
      "    for c,q,h,r in np.ndindex(responses.shape[:-2]): #class,inst,chal,rep\n",
      "        iterator=iterator_matrix[c]\n",
      "        time_delays=time_delay_matrix[c,q]\n",
      "        sigmoid_constants=sigmoid_constant_matrix[c,q]\n",
      "        time_constants=time_constant_matrix[c,q]\n",
      "        predecessors=predecessor_matrix[c]\n",
      "        initial_conditions=initial_condition_matrix[h,r]\n",
      "        hold_times=hold_time_matrix[c]   \n",
      "        x=np.zeros((int(T/dt),N)).astype(np.longdouble)\n",
      "        dx=np.zeros(x.shape[-1]).astype(np.longdouble)\n",
      "        for t in range(x.shape[0]-1):\n",
      "            noise=np.empty(x.shape[1])\n",
      "            for n in range(x.shape[1]):\n",
      "                noise[n]=np.random.random()\n",
      "            noise=noise*noise_scale\n",
      "            edge_index=0\n",
      "            if t<max(hold_times):\n",
      "                for n in range(x.shape[-1]):\n",
      "                    if hold_times[n]>=t:\n",
      "                        x[t,n]=initial_conditions[n]\n",
      "            for i in range(len(iterator)): \n",
      "                n1,n2,deg,mask=iterator[i]\n",
      "                d=-time_delays[edge_index:edge_index+(n2-n1)*deg].reshape((n2-n1,deg))\n",
      "                d+=t\n",
      "                p=predecessors[edge_index:edge_index+(n2-n1)*deg].reshape((n2-n1,deg))\n",
      "                a=sigmoid_constants[n1:n2].reshape((n2-n1,1))\n",
      "                edge_index+=(n2-n1)*deg\n",
      "                y=np.zeros((n2-n1,deg))#.astype(np.longdouble)\n",
      "                for k in range(n2-n1):\n",
      "                    for j in range(deg):\n",
      "                        de=d[k,j]\n",
      "                        pr=p[k,j]\n",
      "                        y[k,j]=x[de,pr]\n",
      "                y=sigmoid(x=y,a=a)\n",
      "                dx[n1:n2]=BOOL(y,mask)\n",
      "            dx=sigmoid(dx,sigmoid_constants)\n",
      "            tau=time_constants[:,0]+(time_constants[:,1]-time_constants[:,0]\n",
      "                                    )*sigmoid(x[t],sigmoid_constants)\n",
      "            dxdt=(-x[t]+dx+noise)/tau\n",
      "        \n",
      "        responses[c,q,h,r]=x[::decimation]\n",
      "                \n",
      "    return responses\n",
      "No export destination, ignored:\n",
      "#export\n",
      "def analyze(responses,\n",
      "           shape = 'design instance challenge repeat time node',\n",
      "           window = 5\n",
      "          ):\n",
      "    '''\n",
      "    Calculates the `puf_statistics` and `boolean_lyapunov` for an array of `responses`.\n",
      "    '''\n",
      "    responses = einops.rearrange(responses,\n",
      "                                 shape+'->design instance challenge repeat time node')\n",
      "    classes,instances,challenges,repeats,T,N=responses.shape\n",
      "    boolean = booleanize(responses).astype(int)\n",
      "    mu_inter=np.empty((classes,T,2))\n",
      "    mu_intra=np.empty((classes,T,2))\n",
      "    delta_mu=np.empty((classes,2))\n",
      "    t_opt=np.empty((classes))\n",
      "    \n",
      "    for s in range(responses.shape[0]):\n",
      "        muinter,muintra,dmuinter,dmuintra,muopt,topt=puf_statistics(\n",
      "            responses=boolean[s],shape='instance challenge repeat time node')\n",
      "        mu_inter[s,:,0]=muinter\n",
      "        mu_inter[s,:,1]=dmuinter\n",
      "        mu_intra[s,:,0]=muintra\n",
      "        mu_intra[s,:,1]=dmuintra\n",
      "        t_opt[s]=topt\n",
      "        delta_mu[s,0]=muopt\n",
      "        delta_mu[s,1]=np.sqrt((muintra[topt]*dmuinter[topt])**2+(muinter[topt]*dmuintra[topt])**2)\n",
      "\n",
      "        \n",
      "    log_dist=np.empty((classes,instances,T-window,2))\n",
      "    lyapunov=np.empty((classes,instances,2))\n",
      "    intercept=np.empty((classes,instances))\n",
      "    \n",
      "    for s in np.ndindex(responses.shape[:2]):\n",
      "        avgln,sigmaln=bool_lyapunov(boolean[s],window,shape= 'challenge repeat time node')\n",
      "        lya,dlya,b=lya_fit(avgln,sigmaln,lstart=0,lend=int(T/2),intercept=True)\n",
      "        a=np.array([avgln,sigmaln])\n",
      "        a=a.T\n",
      "        log_dist[s]=a\n",
      "        a=np.array([lya,dlya])\n",
      "        a=a.T\n",
      "        lyapunov[s]=a\n",
      "        intercept[s]=b\n",
      "    \n",
      "    return dict(mu_inter=mu_inter,mu_intra=mu_intra,delta_mu=delta_mu,t_opt=t_opt,\n",
      "               log_dist=log_dist,lyapunov=lyapunov,intercept=intercept)\n",
      "        \n",
      "No export destination, ignored:\n",
      "#export\n",
      "@sidis.timer\n",
      "def query(classes = 3,\n",
      "         instances = 3,\n",
      "         challenges = 3,\n",
      "         repeats = 3,\n",
      "         inter_variation = 0.1,\n",
      "         decimation = None,\n",
      "         g = (nx.random_regular_graph,3,16),\n",
      "         a = (rng.normal,20,2),\n",
      "         tau = (rng.normal,1,0.1),\n",
      "         f = XOR,\n",
      "         delay = (rng.random,0,1),\n",
      "         edge_replacements = dict(\n",
      "             lengths = 1,\n",
      "             delay = (rng.random,0,0.5),\n",
      "             node_attrs = dict(\n",
      "                 a = (rng.normal,20,2),\n",
      "                 tau = (rng.normal,0.5,0.05),\n",
      "                 f = MPX)\n",
      "         ),\n",
      "         T = 15,\n",
      "         dt = 0.01,\n",
      "         hold = (rng.normal,1,0.1),\n",
      "         noise = 0.01,\n",
      "         enroll = True\n",
      "         ):\n",
      "    '''\n",
      "    Generates an `ensemble` of BooleanNetworks and calculates their `puf_statistics`\n",
      "    and `boolean_lyapunov` with `analyze`.\n",
      "    '''\n",
      "    model_data = ensemble(**{k:v for k,v in locals().items() if k!='enroll'})\n",
      "    if model_data['time_constant_matrix'].shape[-1]==2:\n",
      "        integrator = query_fall\n",
      "    else:\n",
      "        integrator = query_rise\n",
      "    response = integrator(**model_data)\n",
      "    if enroll:\n",
      "        stats = analyze(response)\n",
      "        return response,stats\n",
      "    else:\n",
      "        return response\n",
      "No export destination, ignored:\n",
      "#export\n",
      "def ensemble(classes = 1,\n",
      "             instances = 1,\n",
      "             challenges = 1,\n",
      "             repeats = 1,\n",
      "             inter_variation = 0.,\n",
      "             T = 100,\n",
      "             dt = 0.01,\n",
      "             hold = (rng.normal,1,0),\n",
      "             noise = 0.0,\n",
      "             decimation = None,\n",
      "             g = ring(left=True,right=False,loop=False),\n",
      "             a = np.inf,\n",
      "             tau = 1,\n",
      "             f = NOT,\n",
      "             delay = 0,\n",
      "             edge_replacements = None,\n",
      "             return_classes = True,\n",
      "             steady = True\n",
      "            ):\n",
      "    '''\n",
      "    Generates an ensemble of BooleanNetworks class objects over a distribution,\n",
      "    and an additional set of arrays containing slight perturbations to the parameter\n",
      "    space of each BooleanNetwork. Returns a dictionary containing the characteristic \n",
      "    parameters of each BooleanNetwork as arrays, which are used as the arguments to\n",
      "    a jit accelerated integration routine `query_rise` and `query_fall`.\n",
      "    '''\n",
      "    if decimation is None:\n",
      "        decimation=int(1/dt)\n",
      "    perturb = lambda a: (a*(1.+rng.random(-inter_variation/2.,inter_variation/2.,shape=a.shape))\n",
      "                        ).astype(a.dtype)\n",
      "    \n",
      "    models = [None for c in range(classes)]\n",
      "    hold_times = [None for c in range(classes)]\n",
      "    iterators = [None for c in range(classes)]\n",
      "    predecessors = [None for c in range(classes)]\n",
      "    \n",
      "    time_delays = [[None for i in range(instances)] for c in range(classes)]\n",
      "    sigmoid_constants = [[None for i in range(instances)] for c in range(classes)]\n",
      "    time_constants = [[None for i in range(instances)] for c in range(classes)]\n",
      "\n",
      "    for c in range(classes):\n",
      "        graph = parse_kwargs(g=g)['g']\n",
      "        N = len(graph) #assume all graphs same len\n",
      "        hold_args = tuple(list(hold)+[dict(shape=N)])\n",
      "        model = BooleanNetwork(g=graph,\n",
      "                               init=None,\n",
      "                               hold=parse_kwargs(k=hold_args)['k'],\n",
      "                               a=a,\n",
      "                               T=T,\n",
      "                               dt=dt,\n",
      "                               noise=noise,\n",
      "                               f=f,\n",
      "                               tau=tau,\n",
      "                               delay=delay,\n",
      "                               edge_replacements=edge_replacements,\n",
      "                               plot=False)\n",
      "        models[c] = model\n",
      "        hold_times[c] = model.hold\n",
      "        iterators[c] = model.itr\n",
      "        predecessors[c] = model.predar\n",
      "        \n",
      "        for i in range(instances):\n",
      "            time_delays[c][i] = perturb(model.delay)\n",
      "            sigmoid_constants[c][i] = perturb(model.a)\n",
      "            time_constants[c][i] = perturb(model.tau)\n",
      "            \n",
      "    iterators = np.array(iterators)\n",
      "    predecessors = np.array(predecessors)\n",
      "    time_delays =  np.array(time_delays)\n",
      "    sigmoid_constants = np.array(sigmoid_constants)\n",
      "    time_constants = np.array(time_constants)\n",
      "    hold_times = np.array(hold_times)\n",
      "    \n",
      "    if type(challenges) is int:\n",
      "        initial_conditions = np.empty((challenges,repeats,len(model)))\n",
      "        for h in range(challenges): #assume length of all classes equal \n",
      "            initial_conditions[h,:],_=bool_initial_conditions(model,\n",
      "                        init=rng.random(0,1,absval=True,asint=True,shape=N),\n",
      "                        hold=None,\n",
      "                        steady=steady)\n",
      "    else:\n",
      "        initial_conditions = np.empty((challenges.shape[0],repeats,len(model)))\n",
      "        for h,chal in enumerate(challenges): #assume length of all classes equal \n",
      "            initial_conditions[h,:],_=bool_initial_conditions(model,\n",
      "                        init=chal,\n",
      "                        hold=None,\n",
      "                        steady=steady)\n",
      "\n",
      "    int_type = np.int64\n",
      "    float_type = np.longdouble\n",
      "    data = dict(\n",
      "        iterator_matrix=iterators.astype(int_type),\n",
      "        time_delay_matrix=time_delays.astype(int_type),\n",
      "        sigmoid_constant_matrix=sigmoid_constants.astype(float_type),\n",
      "        time_constant_matrix=time_constants.astype(float_type),\n",
      "        predecessor_matrix=predecessors.astype(int_type),\n",
      "        initial_condition_matrix=initial_conditions.astype(float_type),\n",
      "        hold_time_matrix=hold_times.astype(int_type),\n",
      "        dt=dt,\n",
      "        noise_scale=noise,\n",
      "        T=model.T,\n",
      "        N=len(model),\n",
      "        decimation=decimation\n",
      "    )\n",
      "    if return_classes:\n",
      "        return models,data\n",
      "    else:\n",
      "        return data\n",
      "No export destination, ignored:\n",
      "#export\n",
      "@njit\n",
      "def query_rise(iterator_matrix,\n",
      "               time_delay_matrix,\n",
      "               sigmoid_constant_matrix,\n",
      "               time_constant_matrix,\n",
      "               predecessor_matrix,\n",
      "               initial_condition_matrix,\n",
      "               hold_time_matrix,\n",
      "               dt,\n",
      "               T,\n",
      "               N,\n",
      "               noise_scale,\n",
      "               decimation\n",
      "              ):\n",
      "    '''\n",
      "    Jit accelerated integration routine for the `query` function for an `ensemble` of\n",
      "    `BooleanNetwork`s which have the same rise and fall time. Nearly identical to \n",
      "    `bool_integral` with a loop over `classes,instances,challenges,repeats`. \n",
      "    '''\n",
      "    classes=iterator_matrix.shape[0]\n",
      "    challenges=initial_condition_matrix.shape[0]\n",
      "    repeats=initial_condition_matrix.shape[1]\n",
      "    instances=time_delay_matrix.shape[1]\n",
      "\n",
      "    responses = np.zeros((classes,instances,challenges,repeats,int(T/decimation),N))\n",
      "\n",
      "    for c,q,h,r in np.ndindex(responses.shape[:-2]): #class,inst,chal,rep\n",
      "        iterator=iterator_matrix[c]\n",
      "        time_delays=time_delay_matrix[c,q]\n",
      "        sigmoid_constants=sigmoid_constant_matrix[c,q]\n",
      "        time_constants=time_constant_matrix[c,q]\n",
      "        predecessors=predecessor_matrix[c]\n",
      "        initial_conditions=initial_condition_matrix[h,r]\n",
      "        hold_times=hold_time_matrix[c]   \n",
      "        x=np.zeros((T,N)).astype(np.longdouble)\n",
      "        dx=np.zeros(x.shape[-1]).astype(np.longdouble)\n",
      "        for t in range(x.shape[0]-1):\n",
      "            noise=np.empty(x.shape[1])\n",
      "            for n in range(x.shape[1]):\n",
      "                noise[n]=np.random.random()*noise_scale\n",
      "            #noise=noise*noise_scale\n",
      "            edge_index=0\n",
      "            if t<max(hold_times):\n",
      "                for n in range(x.shape[-1]):\n",
      "                    if hold_times[n]>=t:\n",
      "                        x[t,n]=initial_conditions[n]\n",
      "            for i in range(len(iterator)): \n",
      "                n1,n2,deg,mask=iterator[i]\n",
      "                d=-time_delays[edge_index:edge_index+(n2-n1)*deg].reshape((n2-n1,deg))\n",
      "                d+=t\n",
      "                p=predecessors[edge_index:edge_index+(n2-n1)*deg].reshape((n2-n1,deg))\n",
      "                a=sigmoid_constants[n1:n2].reshape((n2-n1,1))\n",
      "                edge_index+=(n2-n1)*deg\n",
      "                y=np.zeros((n2-n1,deg))#.astype(np.longdouble)\n",
      "                for k in range(n2-n1):\n",
      "                    for j in range(deg):\n",
      "                        de=d[k,j]\n",
      "                        pr=p[k,j]\n",
      "                        y[k,j]=x[de,pr]\n",
      "                y=sigmoid(x=y,a=a)\n",
      "                dx[n1:n2]=BOOL(y,mask)\n",
      "            dx=sigmoid(dx,sigmoid_constants)\n",
      "            dxdt=(-x[t]+dx+noise)/time_constants\n",
      "            x[t+1]=x[t]+dt*dxdt\n",
      "        \n",
      "        responses[c,q,h,r]=x[::decimation]\n",
      "                \n",
      "    return responses\n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-2155445c27e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnotebook2script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nbdev\\export.py\u001b[0m in \u001b[0;36mnotebook2script\u001b[1;34m(fname, silent, to_dict, bare, recursive)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mto_dict\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_mod_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbare\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_notebook2script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbare\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0madd_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lib_path\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nbdev\\export.py\u001b[0m in \u001b[0;36m_notebook2script\u001b[1;34m(fname, modules, silent, to_dict, bare)\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_from_future_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mto_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_add2all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34mf\"'{f}'\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'.'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m         \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr' +$'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMULTILINE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nbdev\\export.py\u001b[0m in \u001b[0;36m_add2all\u001b[1;34m(fname, names, line_width)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0mtw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_indent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsequent_indent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbreak_long_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0mre_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_re__all__def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m     \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mre_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m     \u001b[0mtext_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{text[start:end-1]}{'' if text[end-2]=='[' else ', '}{', '.join(names)}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_all\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'start'"
     ]
    }
   ],
   "source": [
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-grounds",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
